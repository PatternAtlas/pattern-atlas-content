<?xml version="1.1" encoding="UTF-8" standalone="no"?>
<databaseChangeLog xmlns="http://www.liquibase.org/xml/ns/dbchangelog" xmlns:ext="http://www.liquibase.org/xml/ns/dbchangelog-ext" xmlns:pro="http://www.liquibase.org/xml/ns/pro" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.liquibase.org/xml/ns/dbchangelog-ext http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-ext.xsd http://www.liquibase.org/xml/ns/pro http://www.liquibase.org/xml/ns/pro/liquibase-pro-4.6.xsd http://www.liquibase.org/xml/ns/dbchangelog http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-4.6.xsd">
    <changeSet id="1696952262336-1" author="philipp">
        <update tableName="pattern">
            <column name="name" value="Quantum Clustering"/>
            <column name="uri" value="https://patternpedia.org/patternLanguages/reformulatedQuantumComputingPatterns/kmeans"/>
            <column name="content" value="{&quot;Alias&quot;: &quot;&quot;, &quot;Forces&quot;: &quot;Data sets may exhibit non-linear separability, which increases the complexity when clustering. Moreover, data sets utilized in machine learning continuously grow in size, leading to increased training times [[Achiam et al., 2023]](https://doi.org/10.48550/arXiv.2303.08774). Therefore, algorithms whose runtime scales well with the number of data points in the data set and the dimensions of the feature space are required. In addition to the general machine learning forces, also quantum-specific forces have to be taken into account. For example, loading large data sets consisting of many tuples into current quantum devices is difficult due to the high circuit depth of the required state preparation routines [[Akshay et al., 2020]](https://doi.org/10.1103/PhysRevLett.124.090504) [[Preskill, 2018]](https://doi.org/10.22331/q-2018-08-06-79).&quot;, &quot;Intent&quot;: &quot;How to partition a data set into different clusters based on their similarity utilizing a quantum device?&quot;, &quot;Result&quot;: &quot;Utilizing a quantum clustering algorithm may enable identifying clusters in a data set exponentially faster than with a classical clustering algorithm [[Khan et al., 2019c]](https://doi.org/10.48550/arXiv.1909.12183). Often, the computational advantage of quantum clustering algorithms relies on the availability of the input data in a suitable format. Once an implementation of [Quantum Random Access Memory (QRAM) Encoding](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/d9c57511-1101-4707-99bf-36f43a12cb13) is available, data can be encoded efficiently, enabling the full potential of quantum clustering.&quot;, &quot;Context&quot;: &quot;A set of unlabeled data needs to be grouped into different clusters. The clusters should organize the data points according to identified similarities.&quot;, &quot;Examples&quot;: &quot;An exemplary quantum clustering algorithm is the quantum k-means algorithm [Khan et al., 2019b](https://doi.org/10.48550/arXiv.1909.12183) [[Wu et al., 2021]](https://doi.org/10.1007/s11128-021-03384-7): First, k initial data points are randomly selected as centroids for the clustering. Then, the states for both the centroids as well as the remaining data points are prepared. The number k, which corresponds to the number of clusters, can either be specified by the user or automatically determined [[Khan et al., 2019a]](https://doi.org/10.1109/TKDE.2019.2911582). Subsequently, for each data point, the distance to all centroids is calculated utilizing a distance metric, e.g., the Manhattan distance [[Wu et al., 2021]](https://doi.org/10.1007/s11128-021-03384-7) or Euclidean distance [[Khan et al., 2019c]](https://doi.org/10.48550/arXiv.1909.12183). For example, the SWAP test [[Buhrman et al., 2001]](https://doi.org/10.1103/PhysRevLett.87.167902) can be used to determine the Euclidean distances efficiently on a quantum device. Each data point is assigned to the cluster corresponding to the centroid with the smallest distance to the data point. Afterward, the new centroids are calculated classically by computing the mean of all data points assigned to that cluster. If the retrieved centroids differ substantially, i.e., more than a certain threshold specified by the user, from the previous iteration, the previously described procedure is performed again utilizing the new centroids.&quot;, &quot;Solution&quot;: &quot;Figure 2 gives an overview of the general clustering process. Use a quantum device to cluster the m data points $\\newcommand{\\state}[1]{{\\left| #1 \\right&gt;}}\\{\\state{x_i}\\}^m_{i=1}$ of a data set. First, the classical data points are encoded into quantum states  $\\newcommand{\\state}[1]{{\\left| #1 \\right&gt;}}\\{\\state{x_i}\\}^m_{i=1}$ by applying a unitary transformation $U_ϕ$, enabling the quantum computer to process the data. Once the data points are encoded, a given ansatz $V(\\vec{θ})$ is used to calculate the similarity between data points. The ansatz is a parameterized quantum circuit designed to approximate the quantum state that captures the relevant features of the data for similarity measurement. The ansatz computes the similarity either between pairwise data points or between all data points, depending on its structure [Poggiali et al., 2024]. The cost function used in this approach is designed to assign similar points to the same cluster and points with low similarity to different clusters. In the cost function, this is represented by a penalty term that penalizes distant points that are assigned to the same cluster. Additionally, the clustering process is controlled by adding constraints. To ensure that each data point is assigned to exactly one cluster, the following condition must hold $\\sum^{k}_{a=1} q^a_i = 1$. Thereby, classical variables $q^a_i$ are introduced to denote whether a data point $x_i$ is assigned to cluster a. The quantum circuit parameters are updated iteratively to minimize the cost function.\n\n![Solution Sketch](https://raw.githubusercontent.com/PatternAtlas/pattern-atlas-content/refs/heads/main/sketches/quantum_computing_patterns/clustering-solution-sketch.svg)&quot;, &quot;Variants&quot;: &quot;&quot;, &quot;Known Uses&quot;: &quot;Ramirez [[Ramirez, 2024]](https://doi.org/10.69987/) presents different quantum clustering techniques, such as quantum spectral clustering and quantum hierarchical clustering. Kavitha et al. [[Kavitha and Kaulgud, 2023]](https://doi.org/10.1007/s00500-022-07200-x) utilize quantum k-means clustering for detecting heart diseases. Patil et al. [[Patil et al., 2023]](https://doi.org/10.48550/arXiv.2302.00566) introduce two measurement-based quantum clustering algorithms. The first algorithm follows a hierarchical clustering approach. The second algorithm uses unsharp measurements for the clustering process. Gopalakrishnan et al. [[Gopalakrishnan et al., 2024]](https://doi.org/10.3389/frqst.2024.1462004) propose a quantum clustering algorithm that achieves linear scalability with respect to both the number of data points and their density.&quot;, &quot;Related Pattern&quot;: &quot;The\n[Quantum Random Access Memory (QRAM) Encoding](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/d9c57511-1101-4707-99bf-36f43a12cb13) pattern [[Weigold et al., 2021a]](https://doi.org/10.1049/qtc2.12032) can be used to efficiently encode the data points for a quantum device. To facilitate the clustering of complex data sets, the data points can be mapped into a higher dimensional feature space using the [Quantum Kernel Estimator (QKE)](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/5479acf4-0588-49af-a6a9-4956b7ee32af) pattern. The [Quantum Clustering](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/7294c3f2-cb67-492a-8a16-c7384eb9bac2) pattern uses the [Quantum-Classic Split](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/dd15032b-ce2b-40b6-80ac-97623255b531) pattern [[Leymann, 2019]](https://doi.org/10.1007/978-3-030-14082-3_19) to efficiently distribute the computations using quantum and classical hardware and can be realized as a [Hybrid Module](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/4074490a-4236-49ed-83d1-625ce58e2dbd) [[Bühler et al., 2023]](https://www.thinkmind.org/index.php?view=article&amp;articleid=patterns_2023_2_30_70009).&quot;, &quot;Related Patterns&quot;: &quot;This pattern is a refinement of [Quantum-Classic Split](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/dd15032b-ce2b-40b6-80ac-97623255b531) [(Leymann 2019)](https://www.iaas.uni-stuttgart.de/publications/INPROC-2019-05-Towards_a_Pattern_Language_for_Quantum_Algorithms.pdf). \nWithin the quantum computation, [Initialization](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/312bc9d3-26c0-40ae-b90b-56effd136c0d) [(Leymann 2019)](https://www.iaas.uni-stuttgart.de/publications/INPROC-2019-05-Towards_a_Pattern_Language_for_Quantum_Algorithms.pdf) is used.  \n \n&quot;}"/>
            <column name="icon_url" value="https://raw.githubusercontent.com/PatternAtlas/pattern-atlas-content/refs/heads/main/icons/quantum_computing_patterns/quantum_clustering-thin.svg"/>
            <column name="rendered_content" value="{&quot;Alias&quot;: &quot;&quot;, &quot;Forces&quot;: &quot;Data sets may exhibit non-linear separability, which increases the complexity when clustering. Moreover, data sets utilized in machine learning continuously grow in size, leading to increased training times [[Achiam et al., 2023]](https://doi.org/10.48550/arXiv.2303.08774). Therefore, algorithms whose runtime scales well with the number of data points in the data set and the dimensions of the feature space are required. In addition to the general machine learning forces, also quantum-specific forces have to be taken into account. For example, loading large data sets consisting of many tuples into current quantum devices is difficult due to the high circuit depth of the required state preparation routines [[Akshay et al., 2020]](https://doi.org/10.1103/PhysRevLett.124.090504) [[Preskill, 2018]](https://doi.org/10.22331/q-2018-08-06-79).&quot;, &quot;Intent&quot;: &quot;How to partition a data set into different clusters based on their similarity utilizing a quantum device?&quot;, &quot;Result&quot;: &quot;Utilizing a quantum clustering algorithm may enable identifying clusters in a data set exponentially faster than with a classical clustering algorithm [[Khan et al., 2019c]](https://doi.org/10.48550/arXiv.1909.12183). Often, the computational advantage of quantum clustering algorithms relies on the availability of the input data in a suitable format. Once an implementation of [Quantum Random Access Memory (QRAM) Encoding](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/d9c57511-1101-4707-99bf-36f43a12cb13) is available, data can be encoded efficiently, enabling the full potential of quantum clustering.&quot;, &quot;Context&quot;: &quot;A set of unlabeled data needs to be grouped into different clusters. The clusters should organize the data points according to identified similarities.&quot;, &quot;Examples&quot;: &quot;An exemplary quantum clustering algorithm is the quantum k-means algorithm [Khan et al., 2019b](https://doi.org/10.48550/arXiv.1909.12183) [[Wu et al., 2021]](https://doi.org/10.1007/s11128-021-03384-7): First, k initial data points are randomly selected as centroids for the clustering. Then, the states for both the centroids as well as the remaining data points are prepared. The number k, which corresponds to the number of clusters, can either be specified by the user or automatically determined [[Khan et al., 2019a]](https://doi.org/10.1109/TKDE.2019.2911582). Subsequently, for each data point, the distance to all centroids is calculated utilizing a distance metric, e.g., the Manhattan distance [[Wu et al., 2021]](https://doi.org/10.1007/s11128-021-03384-7) or Euclidean distance [[Khan et al., 2019c]](https://doi.org/10.48550/arXiv.1909.12183). For example, the SWAP test [[Buhrman et al., 2001]](https://doi.org/10.1103/PhysRevLett.87.167902) can be used to determine the Euclidean distances efficiently on a quantum device. Each data point is assigned to the cluster corresponding to the centroid with the smallest distance to the data point. Afterward, the new centroids are calculated classically by computing the mean of all data points assigned to that cluster. If the retrieved centroids differ substantially, i.e., more than a certain threshold specified by the user, from the previous iteration, the previously described procedure is performed again utilizing the new centroids.&quot;, &quot;Solution&quot;: &quot;Figure 2 gives an overview of the general clustering process. Use a quantum device to cluster the m data points $\\newcommand{\\state}[1]{{\\left| #1 \\right&gt;}}\\{\\state{x_i}\\}^m_{i=1}$ of a data set. First, the classical data points are encoded into quantum states  $\\newcommand{\\state}[1]{{\\left| #1 \\right&gt;}}\\{\\state{x_i}\\}^m_{i=1}$ by applying a unitary transformation $U_ϕ$, enabling the quantum computer to process the data. Once the data points are encoded, a given ansatz $V(\\vec{θ})$ is used to calculate the similarity between data points. The ansatz is a parameterized quantum circuit designed to approximate the quantum state that captures the relevant features of the data for similarity measurement. The ansatz computes the similarity either between pairwise data points or between all data points, depending on its structure [Poggiali et al., 2024]. The cost function used in this approach is designed to assign similar points to the same cluster and points with low similarity to different clusters. In the cost function, this is represented by a penalty term that penalizes distant points that are assigned to the same cluster. Additionally, the clustering process is controlled by adding constraints. To ensure that each data point is assigned to exactly one cluster, the following condition must hold $\\sum^{k}_{a=1} q^a_i = 1$. Thereby, classical variables $q^a_i$ are introduced to denote whether a data point $x_i$ is assigned to cluster a. The quantum circuit parameters are updated iteratively to minimize the cost function.\n\n![Solution Sketch](https://raw.githubusercontent.com/PatternAtlas/pattern-atlas-content/refs/heads/main/sketches/quantum_computing_patterns/clustering-solution-sketch.svg)&quot;, &quot;Variants&quot;: &quot;&quot;, &quot;Known Uses&quot;: &quot;Ramirez [[Ramirez, 2024]](https://doi.org/10.69987/) presents different quantum clustering techniques, such as quantum spectral clustering and quantum hierarchical clustering. Kavitha et al. [[Kavitha and Kaulgud, 2023]](https://doi.org/10.1007/s00500-022-07200-x) utilize quantum k-means clustering for detecting heart diseases. Patil et al. [[Patil et al., 2023]](https://doi.org/10.48550/arXiv.2302.00566) introduce two measurement-based quantum clustering algorithms. The first algorithm follows a hierarchical clustering approach. The second algorithm uses unsharp measurements for the clustering process. Gopalakrishnan et al. [[Gopalakrishnan et al., 2024]](https://doi.org/10.3389/frqst.2024.1462004) propose a quantum clustering algorithm that achieves linear scalability with respect to both the number of data points and their density.&quot;, &quot;Related Pattern&quot;: &quot;The\n[Quantum Random Access Memory (QRAM) Encoding](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/d9c57511-1101-4707-99bf-36f43a12cb13) pattern [[Weigold et al., 2021a]](https://doi.org/10.1049/qtc2.12032) can be used to efficiently encode the data points for a quantum device. To facilitate the clustering of complex data sets, the data points can be mapped into a higher dimensional feature space using the [Quantum Kernel Estimator (QKE)](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/5479acf4-0588-49af-a6a9-4956b7ee32af) pattern. The [Quantum Clustering](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/7294c3f2-cb67-492a-8a16-c7384eb9bac2) pattern uses the [Quantum-Classic Split](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/dd15032b-ce2b-40b6-80ac-97623255b531) pattern [[Leymann, 2019]](https://doi.org/10.1007/978-3-030-14082-3_19) to efficiently distribute the computations using quantum and classical hardware and can be realized as a [Hybrid Module](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/4074490a-4236-49ed-83d1-625ce58e2dbd) [[Bühler et al., 2023]](https://www.thinkmind.org/index.php?view=article&amp;articleid=patterns_2023_2_30_70009).&quot;, &quot;Related Patterns&quot;: &quot;This pattern is a refinement of [Quantum-Classic Split](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/dd15032b-ce2b-40b6-80ac-97623255b531) [(Leymann 2019)](https://www.iaas.uni-stuttgart.de/publications/INPROC-2019-05-Towards_a_Pattern_Language_for_Quantum_Algorithms.pdf). \nWithin the quantum computation, [Initialization](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/312bc9d3-26c0-40ae-b90b-56effd136c0d) [(Leymann 2019)](https://www.iaas.uni-stuttgart.de/publications/INPROC-2019-05-Towards_a_Pattern_Language_for_Quantum_Algorithms.pdf) is used.  \n \n&quot;}"/>
            <column name="pattern_language_id" value="af7780d5-1f97-4536-8da7-4194b093ab1d"/>
            <column name="paper_ref" value="This pattern is originally published in L. Stiliadou, J. Barzen, M. Beisel, F. Leymann, and B. Weder, Patterns for Quantum Machine Learning, in Proceedings of the 17th International Conference on Pervasive Patterns and Applications (PATTERNS), Xpert Publishing Services (XPS), 2025."/>
            <column name="deployment_modeling_behavior_pattern"/>
            <column name="deployment_modeling_structure_pattern"/>
            <column name="tags" value="algorithm"/>
            <where>id = '7294c3f2-cb67-492a-8a16-c7384eb9bac2'</where>
        </update>
        <update tableName="pattern">
            <column name="name" value="Quantum Classification"/>
            <column name="uri" value="https://patternpedia.org/patternLanguages/reformulatedQuantumComputingPatterns/quantumSupportVectorMachine_qsvm"/>
            <column name="content" value="{&quot;Alias&quot;: &quot;&quot;, &quot;Forces&quot;: &quot;Classifying data is getting increasingly more difficult when the feature space becomes larger [[Havlicek et al., 2019]](https://doi.org/10.1038/s41586-019-0980-2). While quantum computing enables solving this problem by utilizing efficient quantum algorithms, it also leads to additional challenges. For example, high-dimensional data sets can lead to large quantum circuits that may not be successfully executable on today’s Noisy Intermediate-Scale Quantum (NISQ) devices [[Preskill, 2018]](https://doi.org/10.22331/q-2018-08-06-79). Additionally, quantum approaches can suffer from exponential cost concentration, which makes models less sensitive to input data, leading to generalization problems [[Thanasilp et al., 2024]](https://doi.org/10.48550/arXiv.2208.11060) [[Arrasmith et al., 2022]](https://doi.org/10.1088/2058-9565/ac7d06).&quot;, &quot;Intent&quot;: &quot;How to train a classifier to assign new data points to one of multiple classes using a quantum device?&quot;, &quot;Result&quot;: &quot;After the training process, the classifier can be used to assign new data points to one of the existing classes. Utilizing a quantum classifier may enable training a more accurate classifier than using a classical classification technique [[Kavitha and Kaulgud, 2024]](https://doi.org/10.48550/arXiv.2402.08475). Quantum classifiers still function under the influence of noise as they are resistant to a small number of misclassifications [[Havlicek et al., 2019]](https://doi.org/10.1038/s41586-019-0980-2). This is particularly important with the noisiness of today’s quantum devices. However, mitigation mechanisms must be implemented to address the challenges, such as cost concentration in kernel values or flatness in the optimization landscape [[Cerezo et al., 2021]](https://doi.org/10.1038/s42254-021-00348-9) [[Thanasilp et al., 2024]](https://doi.org/10.48550/arXiv.2208.11060).&quot;, &quot;Context&quot;: &quot;New data points need to be classified into one of several different classes. A labeled set of training data is given.&quot;, &quot;Examples&quot;: &quot;An example is the kernel-based quantum support vector machine [[Rebentrost et al., 2014]](https://doi.org/10.1103/PhysRevLett.113.130503), achieving logarithmic complexity with respect to both the data dimension and the number of training examples. Another example is the variational quantum support vector machine [[Havlicek et al., 2019]](https://doi.org/10.1038/s41586-019-0980-2) [[Gentinetta et al., 2024]](\thttps://doi.org/10.22331/q-2024-01-11-1225), which uses a parameterized quantum circuit to directly implement a SVM on a QPU.&quot;, &quot;Solution&quot;: &quot;Train a classifier using a quantum device to classify new data points precisely. In Figure 3, an overview of two different approaches for training a classifier is depicted. Classifiers can either be trained using (i) a kernel-based method or (ii) a variational method. Generally, the input for training a classifier is an initial set of labeled data $\\{(x_i, y_i)\\}^n_{i=1}$, where $x_i$ are the feature vectors, $y_i$ are the labels, i.e., real numbers, and n is the size of the training set. In the kernel-based approach, a quantum kernel is used to measure the similarities between data points by mapping them into a high-dimensional Hilbert space and computing the inner product of their corresponding quantum states. This quantum kernel is computed for all pairs of training data by applying a unitary $U_ϕ(x_i)$ to encode each data point xi into a quantum state. The adjoint operation $U^{\\dagger}_ϕ(x_j)$ is then applied to calculate the overlap between the states corresponding to $x_i$ and $x_j$. Then, a classical algorithm, e.g., a classical support vector machine  Burges, 1998], is used for computing the classifier based on the previously calculated kernel. Alternatively, the variational method optimizes the parameters of a quantum circuit to directly realize the classifier. In this approach, a data point x is first encoded into a quantum state using a unitary $U_ϕ(x)$, which maps the classical data into a quantum state. Once the data are encoded, a parameterized quantum circuit is applied. The circuit produces an output whose expectation value &lt; V &gt; determines the predicted label for a given data point x. The parameters of the circuit are iteratively optimized by a classical optimizer that minimizes a quantum cost function that calculates the differences between the predicted and actual labels from the data set. \n\n![Solution Sketch](https://raw.githubusercontent.com/PatternAtlas/pattern-atlas-content/refs/heads/main/sketches/quantum_computing_patterns/classification-solution-sketch.svg)&quot;, &quot;Variants&quot;: &quot;&quot;, &quot;Known Uses&quot;: &quot;To train the quantum classifier, different approaches can be used, e.g., variational quantum support vector machines [[Ramirez, 2024]](https://doi.org/10.69987/) [[Havlicek et al., 2019]](https://doi.org/10.1038/s41586-019-0980-2), quantum decision trees [[Lu and Braunstein, 2014]](https://doi.org/10.1007/s11128-013-0687-5), and quantum nearest neighbor classification [[Roga et al., 2023]](https://doi.org/10.48550/arXiv.2307.03396). Furthermore, quantum classifiers have been applied in different application areas, e.g., image recognition [[Li et al., 2024]](https://www.computer.org/csdl/journal/tc/2024/09/10564586/1XVDnnuBJG8), analyzing the sentiments of sentences [[Ruskanda et al., 2023]](https://ieeexplore.ieee.org/document/10216989), and predicting air pollution [[Farooq et al., 2024]](https://doi.org/10.1038/s41598-024-69663-2).&quot;, &quot;Related Pattern&quot;: &quot;The [Quantum Random Access Memory (QRAM) Encoding](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/d9c57511-1101-4707-99bf-36f43a12cb13) pattern [[Weigold et al., 2021a]](https://doi.org/10.1049/qtc2.12032) can be utilized to achieve a speed-up when encoding data. A quantum classifier can be realized using a [Variational Quantum Algorithm (VQA)](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/bc795a9b-7977-4e01-b513-f9f5aba38aa7) [[Weigold et al., 2021b]](https://doi.org/10.1007/978-3-030-87568-8_2).&quot;, &quot;Related Patterns&quot;: &quot;This pattern is a refinement of [Quantum-Classic Split](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/dd15032b-ce2b-40b6-80ac-97623255b531) [(Leymann 2019)](https://www.iaas.uni-stuttgart.de/publications/INPROC-2019-05-Towards_a_Pattern_Language_for_Quantum_Algorithms.pdf). \nWithin the quantum computation, [Initialization](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/312bc9d3-26c0-40ae-b90b-56effd136c0d) [(Leymann 2019)](https://www.iaas.uni-stuttgart.de/publications/INPROC-2019-05-Towards_a_Pattern_Language_for_Quantum_Algorithms.pdf) is used.  \n \n&quot;}"/>
            <column name="icon_url" value="https://raw.githubusercontent.com/PatternAtlas/pattern-atlas-content/refs/heads/main/icons/quantum_computing_patterns/quantum_classification-thin.svg"/>
            <column name="rendered_content" value="{&quot;Alias&quot;: &quot;&quot;, &quot;Forces&quot;: &quot;Classifying data is getting increasingly more difficult when the feature space becomes larger [[Havlicek et al., 2019]](https://doi.org/10.1038/s41586-019-0980-2). While quantum computing enables solving this problem by utilizing efficient quantum algorithms, it also leads to additional challenges. For example, high-dimensional data sets can lead to large quantum circuits that may not be successfully executable on today’s Noisy Intermediate-Scale Quantum (NISQ) devices [[Preskill, 2018]](https://doi.org/10.22331/q-2018-08-06-79). Additionally, quantum approaches can suffer from exponential cost concentration, which makes models less sensitive to input data, leading to generalization problems [[Thanasilp et al., 2024]](https://doi.org/10.48550/arXiv.2208.11060) [[Arrasmith et al., 2022]](https://doi.org/10.1088/2058-9565/ac7d06).&quot;, &quot;Intent&quot;: &quot;How to train a classifier to assign new data points to one of multiple classes using a quantum device?&quot;, &quot;Result&quot;: &quot;After the training process, the classifier can be used to assign new data points to one of the existing classes. Utilizing a quantum classifier may enable training a more accurate classifier than using a classical classification technique [[Kavitha and Kaulgud, 2024]](https://doi.org/10.48550/arXiv.2402.08475). Quantum classifiers still function under the influence of noise as they are resistant to a small number of misclassifications [[Havlicek et al., 2019]](https://doi.org/10.1038/s41586-019-0980-2). This is particularly important with the noisiness of today’s quantum devices. However, mitigation mechanisms must be implemented to address the challenges, such as cost concentration in kernel values or flatness in the optimization landscape [[Cerezo et al., 2021]](https://doi.org/10.1038/s42254-021-00348-9) [[Thanasilp et al., 2024]](https://doi.org/10.48550/arXiv.2208.11060).&quot;, &quot;Context&quot;: &quot;New data points need to be classified into one of several different classes. A labeled set of training data is given.&quot;, &quot;Examples&quot;: &quot;An example is the kernel-based quantum support vector machine [[Rebentrost et al., 2014]](https://doi.org/10.1103/PhysRevLett.113.130503), achieving logarithmic complexity with respect to both the data dimension and the number of training examples. Another example is the variational quantum support vector machine [[Havlicek et al., 2019]](https://doi.org/10.1038/s41586-019-0980-2) [[Gentinetta et al., 2024]](\thttps://doi.org/10.22331/q-2024-01-11-1225), which uses a parameterized quantum circuit to directly implement a SVM on a QPU.&quot;, &quot;Solution&quot;: &quot;Train a classifier using a quantum device to classify new data points precisely. In Figure 3, an overview of two different approaches for training a classifier is depicted. Classifiers can either be trained using (i) a kernel-based method or (ii) a variational method. Generally, the input for training a classifier is an initial set of labeled data $\\{(x_i, y_i)\\}^n_{i=1}$, where $x_i$ are the feature vectors, $y_i$ are the labels, i.e., real numbers, and n is the size of the training set. In the kernel-based approach, a quantum kernel is used to measure the similarities between data points by mapping them into a high-dimensional Hilbert space and computing the inner product of their corresponding quantum states. This quantum kernel is computed for all pairs of training data by applying a unitary $U_ϕ(x_i)$ to encode each data point xi into a quantum state. The adjoint operation $U^{\\dagger}_ϕ(x_j)$ is then applied to calculate the overlap between the states corresponding to $x_i$ and $x_j$. Then, a classical algorithm, e.g., a classical support vector machine  Burges, 1998], is used for computing the classifier based on the previously calculated kernel. Alternatively, the variational method optimizes the parameters of a quantum circuit to directly realize the classifier. In this approach, a data point x is first encoded into a quantum state using a unitary $U_ϕ(x)$, which maps the classical data into a quantum state. Once the data are encoded, a parameterized quantum circuit is applied. The circuit produces an output whose expectation value &lt; V &gt; determines the predicted label for a given data point x. The parameters of the circuit are iteratively optimized by a classical optimizer that minimizes a quantum cost function that calculates the differences between the predicted and actual labels from the data set. \n\n![Solution Sketch](https://raw.githubusercontent.com/PatternAtlas/pattern-atlas-content/refs/heads/main/sketches/quantum_computing_patterns/classification-solution-sketch.svg)&quot;, &quot;Variants&quot;: &quot;&quot;, &quot;Known Uses&quot;: &quot;To train the quantum classifier, different approaches can be used, e.g., variational quantum support vector machines [[Ramirez, 2024]](https://doi.org/10.69987/) [[Havlicek et al., 2019]](https://doi.org/10.1038/s41586-019-0980-2), quantum decision trees [[Lu and Braunstein, 2014]](https://doi.org/10.1007/s11128-013-0687-5), and quantum nearest neighbor classification [[Roga et al., 2023]](https://doi.org/10.48550/arXiv.2307.03396). Furthermore, quantum classifiers have been applied in different application areas, e.g., image recognition [[Li et al., 2024]](https://www.computer.org/csdl/journal/tc/2024/09/10564586/1XVDnnuBJG8), analyzing the sentiments of sentences [[Ruskanda et al., 2023]](https://ieeexplore.ieee.org/document/10216989), and predicting air pollution [[Farooq et al., 2024]](https://doi.org/10.1038/s41598-024-69663-2).&quot;, &quot;Related Pattern&quot;: &quot;The [Quantum Random Access Memory (QRAM) Encoding](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/d9c57511-1101-4707-99bf-36f43a12cb13) pattern [[Weigold et al., 2021a]](https://doi.org/10.1049/qtc2.12032) can be utilized to achieve a speed-up when encoding data. A quantum classifier can be realized using a [Variational Quantum Algorithm (VQA)](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/bc795a9b-7977-4e01-b513-f9f5aba38aa7) [[Weigold et al., 2021b]](https://doi.org/10.1007/978-3-030-87568-8_2).&quot;, &quot;Related Patterns&quot;: &quot;This pattern is a refinement of [Quantum-Classic Split](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/dd15032b-ce2b-40b6-80ac-97623255b531) [(Leymann 2019)](https://www.iaas.uni-stuttgart.de/publications/INPROC-2019-05-Towards_a_Pattern_Language_for_Quantum_Algorithms.pdf). \nWithin the quantum computation, [Initialization](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/312bc9d3-26c0-40ae-b90b-56effd136c0d) [(Leymann 2019)](https://www.iaas.uni-stuttgart.de/publications/INPROC-2019-05-Towards_a_Pattern_Language_for_Quantum_Algorithms.pdf) is used.  \n \n&quot;}"/>
            <column name="pattern_language_id" value="af7780d5-1f97-4536-8da7-4194b093ab1d"/>
            <column name="paper_ref" value="This pattern is originally published in L. Stiliadou, J. Barzen, M. Beisel, F. Leymann, and B. Weder, Patterns for Quantum Machine Learning, in Proceedings of the 17th International Conference on Pervasive Patterns and Applications (PATTERNS), Xpert Publishing Services (XPS), 2025."/>
            <column name="deployment_modeling_behavior_pattern"/>
            <column name="deployment_modeling_structure_pattern"/>
            <column name="tags" value="algorithm"/>            
            <where>id = 'cc3731cf-ecbb-490f-b996-525c8f60d8a9'</where>
        </update>
        <insert tableName="pattern">
            <column name="id" value="726ead05-e582-4704-904b-6ce2174399d8"/>
            <column name="name" value="Quantum Neural Network (QNN)"/>
            <column name="uri" value="https://patternpedia.org/patternLanguages/quantumAlgorithmPatterns/quantumNeuralNetwork"/>
            <column name="content" value="{&quot;Forces&quot;: &quot;Identifying a unitary operator that is capable of mapping input data to their respective output is getting increasingly more difficult with the complexity and variety of the data. Determining such a mapping requires a lot of input data, and the training procedure requires significant computational power [[Achiam et al., 2023]](https://doi.org/10.48550/arXiv.2303.08774). However, this number can be reduced as outlined by the Quantum No-Free-Lunch Theorem since only obtaining a subset of the training samples as entangled quantum states is already beneficial [[Mandl et al., 2023]](https://doi.org/10.48550/arXiv.2309.13711).&quot;, &quot;Intent&quot;: &quot;How to learn an unknown unitary operator using a quantum device?&quot;, &quot;Result&quot;: &quot;The result is a set of parameters that configures the QNN to approximate the expected output of the unknown unitary operator. The quality of the approximation depends on the size of the training set, its linear structure, and the degree of entanglement [[Mandl et al., 2024]](https://doi.org/10.1007/978-981-97-0989-2_12). While entangled data provides benefits for the training of QNNs, a too high level of entanglement can lead to barren plateaus [[Thanasilp et al., 2023b]](https://doi.org/10.1007/s42484-023-00103-6).&quot;, &quot;Context&quot;: &quot;An unknown unitary operator needs to be learned from a training set containing the quantum inputs and the expected quantum outputs.&quot;, &quot;Examples&quot;: &quot;A special kind of QNNs are quantum convolutional neural networks, which are utilized for processing structured grid data, e.g., for image processing [[Hur et al., 2022]](https://doi.org/10.1007/s42484-021-00061-x) [[Wei et al., 2022]](https://doi.org/10.1007/s43673-021-00030-3). It comprises four different types of layers: (i) First, state preparation layers are used to encode the classical input data. (ii) The convolutional layers enable the detection of spatial patterns within the input data. (iii) Pooling layers reduce some of the spatial dimensions to focus on the optimization of the most important features. (iv) Finally, fully connected layers are used to produce the final output of the quantum convolutional network. &quot;, &quot;Solution&quot;: &quot;Figure 4 shows the training process of a QNN to learn an unknown unitary operator $U$: To realize a corresponding quantum circuit, first, the input data are encoded to a quantum state $\\newcommand{\\state}[1]{{\\left| #1 \\right&gt;}} \\state{x}$. Similarly to classical neural networks, quantum circuits realizing a QNN comprise various parameterized hidden layers $V_i(\\vec{θ})$ to approximate $U$. The parameters $\\vec{θ}$ are iteratively adjusted by an optimizer, which minimizes a cost function until the quantum circuit produces approximately the expected outputs. The cost function uses the expected outputs and similarity measures, such as fidelity, to evaluate how closely the produced outputs $\\newcommand{\\state}[1]{{\\left| #1 \\right&gt;}} \\state{\\tilde{y}}$ match the expected ones $\\newcommand{\\state}[1]{{\\left| #1 \\right&gt;}} \\state{y}$.\n\n![Solution Sketch](https://raw.githubusercontent.com/PatternAtlas/pattern-atlas-content/refs/heads/main/sketches/quantum_computing_patterns/qnn-solution-sketch.svg)\n\n&quot;, &quot;Known Uses&quot;: &quot;Jeswal et al. [[Jeswal and Chakraverty, 2019]](https://doi.org/10.1007/s11831-018-9269-0) and Vasuki et al. [[Vasuki et al., 2023]](https://quingpublications.com/journals/index.php/ijirse/article/view/129/125) provide surveys overviewing the various application areas of QNNs, ranging from prediction to pattern recognition problems. Kashif et al. [[Kashif and Shafique, 2024]](https://doi.org/10.48550/arXiv.2402.08475) present an approach for efficiently training QNNs in the presence of noise when using NISQ devices.&quot;, &quot;Related Pattern&quot;: &quot;Quantum neural networks are a realization of the [Variational Quantum Algorithm (VQA)](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/bc795a9b-7977-4e01-b513-f9f5aba38aa7) pattern [[Weigold et al., 2021b]](https://doi.org/10.1007/978-3-030-87568-8_2). Different state preparation routines, such as [Angle Encoding](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/e595558d-bfea-4b82-9f47-a38a2097b245) or [Basis Encoding](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/bcd4c7a1-3c92-4f8c-a530-72b8b95d3750) [[Weigold et al., 2021a]](https://doi.org/10.1049/qtc2.12032), can be utilized to encode the input data of the QNN. To integrate a QNN in existing applications, it can be provided as a [Hybrid Module](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/4074490a-4236-49ed-83d1-625ce58e2dbd) [[Bühler et al., 2023]](https://www.thinkmind.org/index.php?view=article&amp;articleid=patterns_2023_2_30_70009).&quot;}"/>
            <column name="icon_url" value="https://raw.githubusercontent.com/PatternAtlas/pattern-atlas-content/refs/heads/main/icons/quantum_computing_patterns/quantum_neural_network-thin.svg"/>
            <column name="rendered_content" value="{&quot;Forces&quot;: &quot;Identifying a unitary operator that is capable of mapping input data to their respective output is getting increasingly more difficult with the complexity and variety of the data. Determining such a mapping requires a lot of input data, and the training procedure requires significant computational power [[Achiam et al., 2023]](https://doi.org/10.48550/arXiv.2303.08774). However, this number can be reduced as outlined by the Quantum No-Free-Lunch Theorem since only obtaining a subset of the training samples as entangled quantum states is already beneficial [[Mandl et al., 2023]](https://doi.org/10.48550/arXiv.2309.13711).&quot;, &quot;Intent&quot;: &quot;How to learn an unknown unitary operator using a quantum device?&quot;, &quot;Result&quot;: &quot;The result is a set of parameters that configures the QNN to approximate the expected output of the unknown unitary operator. The quality of the approximation depends on the size of the training set, its linear structure, and the degree of entanglement [[Mandl et al., 2024]](https://doi.org/10.1007/978-981-97-0989-2_12). While entangled data provides benefits for the training of QNNs, a too high level of entanglement can lead to barren plateaus [[Thanasilp et al., 2023b]](https://doi.org/10.1007/s42484-023-00103-6).&quot;, &quot;Context&quot;: &quot;An unknown unitary operator needs to be learned from a training set containing the quantum inputs and the expected quantum outputs.&quot;, &quot;Examples&quot;: &quot;A special kind of QNNs are quantum convolutional neural networks, which are utilized for processing structured grid data, e.g., for image processing [[Hur et al., 2022]](https://doi.org/10.1007/s42484-021-00061-x) [[Wei et al., 2022]](https://doi.org/10.1007/s43673-021-00030-3). It comprises four different types of layers: (i) First, state preparation layers are used to encode the classical input data. (ii) The convolutional layers enable the detection of spatial patterns within the input data. (iii) Pooling layers reduce some of the spatial dimensions to focus on the optimization of the most important features. (iv) Finally, fully connected layers are used to produce the final output of the quantum convolutional network. &quot;, &quot;Solution&quot;: &quot;Figure 4 shows the training process of a QNN to learn an unknown unitary operator $U$: To realize a corresponding quantum circuit, first, the input data are encoded to a quantum state $\\newcommand{\\state}[1]{{\\left| #1 \\right&gt;}} \\state{x}$. Similarly to classical neural networks, quantum circuits realizing a QNN comprise various parameterized hidden layers $V_i(\\vec{θ})$ to approximate $U$. The parameters $\\vec{θ}$ are iteratively adjusted by an optimizer, which minimizes a cost function until the quantum circuit produces approximately the expected outputs. The cost function uses the expected outputs and similarity measures, such as fidelity, to evaluate how closely the produced outputs $\\newcommand{\\state}[1]{{\\left| #1 \\right&gt;}} \\state{\\tilde{y}}$ match the expected ones $\\newcommand{\\state}[1]{{\\left| #1 \\right&gt;}} \\state{y}$.\n\n![Solution Sketch](https://raw.githubusercontent.com/PatternAtlas/pattern-atlas-content/refs/heads/main/sketches/quantum_computing_patterns/qnn-solution-sketch.svg)\n\n&quot;, &quot;Known Uses&quot;: &quot;Jeswal et al. [[Jeswal and Chakraverty, 2019]](https://doi.org/10.1007/s11831-018-9269-0) and Vasuki et al. [[Vasuki et al., 2023]](https://quingpublications.com/journals/index.php/ijirse/article/view/129/125) provide surveys overviewing the various application areas of QNNs, ranging from prediction to pattern recognition problems. Kashif et al. [[Kashif and Shafique, 2024]](https://doi.org/10.48550/arXiv.2402.08475) present an approach for efficiently training QNNs in the presence of noise when using NISQ devices.&quot;, &quot;Related Pattern&quot;: &quot;Quantum neural networks are a realization of the [Variational Quantum Algorithm (VQA)](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/bc795a9b-7977-4e01-b513-f9f5aba38aa7) pattern [[Weigold et al., 2021b]](https://doi.org/10.1007/978-3-030-87568-8_2). Different state preparation routines, such as [Angle Encoding](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/e595558d-bfea-4b82-9f47-a38a2097b245) or [Basis Encoding](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/bcd4c7a1-3c92-4f8c-a530-72b8b95d3750) [[Weigold et al., 2021a]](https://doi.org/10.1049/qtc2.12032), can be utilized to encode the input data of the QNN. To integrate a QNN in existing applications, it can be provided as a [Hybrid Module](pattern-languages/af7780d5-1f97-4536-8da7-4194b093ab1d/4074490a-4236-49ed-83d1-625ce58e2dbd) [[Bühler et al., 2023]](https://www.thinkmind.org/index.php?view=article&amp;articleid=patterns_2023_2_30_70009).&quot;}"/>
            <column name="pattern_language_id" value="af7780d5-1f97-4536-8da7-4194b093ab1d"/>
            <column name="paper_ref" value="This pattern is originally published in L. Stiliadou, J. Barzen, M. Beisel, F. Leymann, and B. Weder, Patterns for Quantum Machine Learning, in Proceedings of the 17th International Conference on Pervasive Patterns and Applications (PATTERNS), Xpert Publishing Services (XPS), 2025."/>
            <column name="deployment_modeling_behavior_pattern"/>
            <column name="deployment_modeling_structure_pattern"/>
            <column name="tags" value="algorithm"/>
        </insert>
        <insert tableName="undirected_edge">
            <column name="id" value="185e580a-5eb4-4658-bb4d-ed832f8484ae"/>
            <column name="description" value="null"/>
            <column name="type" value="canBeUsedWith"/>
            <column name="p1_id" value="d9c57511-1101-4707-99bf-36f43a12cb13"/>
            <column name="p2_id" value="cc3731cf-ecbb-490f-b996-525c8f60d8a9"/>
            <column name="pattern_language_id" value="af7780d5-1f97-4536-8da7-4194b093ab1d"/>
        </insert>
        <insert tableName="undirected_edge">
            <column name="id" value="cbde7b8e-78b1-4ed0-ae05-e08a581c7f9d"/>
            <column name="description" value="null"/>
            <column name="type" value="canBeUsedWith"/>
            <column name="p1_id" value="bc795a9b-7977-4e01-b513-f9f5aba38aa7"/>
            <column name="p2_id" value="cc3731cf-ecbb-490f-b996-525c8f60d8a9"/>
            <column name="pattern_language_id" value="af7780d5-1f97-4536-8da7-4194b093ab1d"/>
        </insert>

        <insert tableName="undirected_edge">
            <column name="id" value="54cde3b8-1047-4a2d-a8bf-85ed9c49f0b8"/>
            <column name="description" value="null"/>
            <column name="type" value="isRelatedTo"/>
            <column name="p1_id" value="bc795a9b-7977-4e01-b513-f9f5aba38aa7"/>
            <column name="p2_id" value="726ead05-e582-4704-904b-6ce2174399d8"/>
            <column name="pattern_language_id" value="af7780d5-1f97-4536-8da7-4194b093ab1d"/>
        </insert>
        <insert tableName="undirected_edge">
            <column name="id" value="c6cd023a-1c88-41c5-9456-b78cee98bac8"/>
            <column name="description" value="null"/>
            <column name="type" value="canBeUsedWith"/>
            <column name="p1_id" value="e595558d-bfea-4b82-9f47-a38a2097b245"/>
            <column name="p2_id" value="726ead05-e582-4704-904b-6ce2174399d8"/>
            <column name="pattern_language_id" value="af7780d5-1f97-4536-8da7-4194b093ab1d"/>
        </insert>
        <insert tableName="undirected_edge">
            <column name="id" value="21e3111e-5578-4923-ad23-5194e1179531"/>
            <column name="description" value="null"/>
            <column name="type" value="canBeUsedWith"/>
            <column name="p1_id" value="bcd4c7a1-3c92-4f8c-a530-72b8b95d3750"/>
            <column name="p2_id" value="726ead05-e582-4704-904b-6ce2174399d8"/>
            <column name="pattern_language_id" value="af7780d5-1f97-4536-8da7-4194b093ab1d"/>
        </insert>

        <insert tableName="directed_edge">
            <column name="id" value="a0492567-9cb2-475f-b1aa-aebd4f09fc85"/>
            <column name="description" value="null"/>
            <column name="type" value="isRelatedTo"/>
            <column name="pattern_language_id" value="af7780d5-1f97-4536-8da7-4194b093ab1d"/>
            <column name="source_id" value="726ead05-e582-4704-904b-6ce2174399d8"/>
            <column name="target_id" value="4074490a-4236-49ed-83d1-625ce58e2dbd"/>
        </insert>
        <insert tableName="directed_edge">
            <column name="id" value="3b57fc00-db9e-4431-9ce2-1574f890f58b"/>
            <column name="description" value="null"/>
            <column name="type" value="isRelatedTo"/>
            <column name="pattern_language_id" value="af7780d5-1f97-4536-8da7-4194b093ab1d"/>
            <column name="source_id" value="7294c3f2-cb67-492a-8a16-c7384eb9bac2"/>
            <column name="target_id" value="d9c57511-1101-4707-99bf-36f43a12cb13"/>
        </insert>
        <insert tableName="directed_edge">
            <column name="id" value="20b659ff-8b59-48bc-8501-e0972a9deacc"/>
            <column name="description" value="null"/>
            <column name="type" value="canBeUsedWith"/>
            <column name="pattern_language_id" value="af7780d5-1f97-4536-8da7-4194b093ab1d"/>
            <column name="source_id" value="7294c3f2-cb67-492a-8a16-c7384eb9bac2"/>
            <column name="target_id" value="5479acf4-0588-49af-a6a9-4956b7ee32af"/>
        </insert>
        <insert tableName="directed_edge">
            <column name="id" value="2c68e93c-fc82-4034-8571-1156b5543661"/>
            <column name="description" value="null"/>
            <column name="type" value="isRelatedTo"/>
            <column name="pattern_language_id" value="af7780d5-1f97-4536-8da7-4194b093ab1d"/>
            <column name="source_id" value="7294c3f2-cb67-492a-8a16-c7384eb9bac2"/>
            <column name="target_id" value="dd15032b-ce2b-40b6-80ac-97623255b531"/>
        </insert>
        <insert tableName="directed_edge">
            <column name="id" value="a7d0523f-d8ca-49d5-9824-3c3dc8357118"/>
            <column name="description" value="null"/>
            <column name="type" value="isRelatedTo"/>
            <column name="pattern_language_id" value="af7780d5-1f97-4536-8da7-4194b093ab1d"/>
            <column name="source_id" value="7294c3f2-cb67-492a-8a16-c7384eb9bac2"/>
            <column name="target_id" value="4074490a-4236-49ed-83d1-625ce58e2dbd"/>
        </insert>

    </changeSet>
</databaseChangeLog>
